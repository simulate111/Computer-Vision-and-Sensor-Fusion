{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Computer-Vision-and-Sensor-Fusion/blob/main/Assignment_2_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAQr-XfCAziO"
      },
      "source": [
        "### Add libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Sz2NL_1RAziS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.resnet import ResNet50\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8t9G4XaAziW"
      },
      "source": [
        "### Path system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nud0wxdmAziX",
        "outputId": "f1ca3ff7-ecd0-42fc-e872-e071810fb0fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DIR] The directory of the current dataset is /Datasets/weather_dataset\n"
          ]
        }
      ],
      "source": [
        "# you need the current working directory NB: works both windows and linux\n",
        "current_working_directory = os.getcwd()\n",
        "current_working_directory = os.path.dirname(current_working_directory)\n",
        "\n",
        "# get the directory where I want to download the dataset\n",
        "path_of_download = os.path.join(*['..', current_working_directory, 'Datasets', 'weather_dataset'])\n",
        "print(f\"[DIR] The directory of the current dataset is {path_of_download}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa2gHfK5AziY"
      },
      "source": [
        "### function for data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eIhPiXr3AziY"
      },
      "outputs": [],
      "source": [
        "# here let s do some functions that we can re-use also for other assignment\n",
        "def load_the_data_and_the_labels(data_set_path: str, target_size: tuple or None = None):\n",
        "    try:\n",
        "        dataset, labels, name_of_the_labels = list(), list(), list()\n",
        "        # let s loop here and we try to discover how many class we have\n",
        "        for class_number, class_name in enumerate(os.listdir(data_set_path)):\n",
        "            full_path_the_data = os.path.join(data_set_path, class_name)\n",
        "            print(f\"[WALK] I am walking into {full_path_the_data}\")\n",
        "\n",
        "            # add the list to nam _list\n",
        "            name_of_the_labels.append(class_name)\n",
        "\n",
        "            for single_image in os.listdir(f\"{full_path_the_data}\"):\n",
        "                full_path_to_image = os.path.join(*[full_path_the_data, single_image])\n",
        "\n",
        "                # add the class number\n",
        "                labels.append(class_number)\n",
        "\n",
        "                if target_size is None:\n",
        "                    # let s load the image\n",
        "                    image = tf.keras.utils.load_img(full_path_to_image)\n",
        "                else:\n",
        "                    image = tf.keras.utils.load_img(full_path_to_image, target_size=target_size)\n",
        "\n",
        "                # transform PIL object in image\n",
        "                image = tf.keras.utils.img_to_array(image)\n",
        "\n",
        "                # add the image to the ds list\n",
        "                dataset.append(image)\n",
        "\n",
        "        return np.array(dataset, dtype='uint8'), np.array(labels, dtype='int'), name_of_the_labels\n",
        "    except Exception as ex:\n",
        "        print(f\"[EXCEPTION] load the data and the labels throws exceptions {ex}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlJhk3N_Azia"
      },
      "source": [
        "### OHE function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8uTXiW-qAzic"
      },
      "outputs": [],
      "source": [
        "# here we have to one hot encode the labes\n",
        "def make_the_one_hot_encoding(labels_to_transform):\n",
        "    try:\n",
        "        enc = OneHotEncoder(handle_unknown='ignore')\n",
        "        # this is a trick to figure the array as 2d array instead of list\n",
        "        temp = np.reshape(labels_to_transform, (-1, 1))\n",
        "        labels_to_transform = enc.fit_transform(temp).toarray()\n",
        "        print(f'[ONE HOT ENCODING] Labels are one-hot-encoded: {(labels_to_transform.sum(axis=1) - np.ones(labels_to_transform.shape[0])).sum() == 0}')\n",
        "        return labels_to_transform\n",
        "    except Exception as ex:\n",
        "        print(f\"[EXCEPTION] Make the one hot encoding throws exception {ex}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYoAaWjcAzid"
      },
      "source": [
        "### load the data and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2_dJnLX4Azie"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaPeBtYCAzie"
      },
      "source": [
        "### normalize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8Hsxpc2EAzif"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAS0xXyOAzig"
      },
      "source": [
        "### split the dataset in train and test set (ratio 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hsLGkzzkAzig"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mJZkxdKAzig"
      },
      "source": [
        "### create the CNN and set all parameters to trainable\n",
        "a.\tInput layer\n",
        "b.\tAs base model use VGG19:\n",
        "    i.\tWeights: imagenet\n",
        "    ii.\tInclude_top: False\n",
        "    iii.\tInput_shape the target shape described in point 1.\n",
        "c.\tAdd a flatten layer\n",
        "d.\tAdd a Dense layer with 512 units and a dropout layer with 0.1 unit.\n",
        "e.\tAdd a Dense layer with 256 units and a dropout layer with 0.1 unit.\n",
        "f.\tAdd the final classifier with the correct number of units and the suitable activation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ycvLwmPVAzig"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qdj0IV7oAzih"
      },
      "source": [
        "### compile the model with adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fZ-cvQWsAzih"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olm0gBGwAzih"
      },
      "source": [
        "### Fit the model with batch size 32 and 15 epochs (This take 15 - 20 minutes with the CPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zsHiZOoiAzih"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFKbkhyFAzih"
      },
      "source": [
        "### Evaluate  the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nI3ucEjLAzih"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvlS0DyiAzii"
      },
      "source": [
        "#### Make and show predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jthngnE8Azii"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcYypGbAAzii"
      },
      "source": [
        "### make confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cc0nAirIAzii"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f_pJSVrAzii"
      },
      "source": [
        "### Load again the cnn but this time set the parameters to NOT TRAINABLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ja-xZS7JAzii"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5orAaxQNAzii"
      },
      "source": [
        "### Fit the model with batch size 32 and 15 epochs (This is fsaster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qaNRNE7kAzii"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R9F8Iq9Azii"
      },
      "source": [
        "# Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rWewSL9RAzij"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22tmstVcAzij"
      },
      "source": [
        "### Make and show some predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cLlDeAlWAzij"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dl_course_2024",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}