{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Computer-Vision-and-Sensor-Fusion/blob/main/Assignment4_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g_aMY60QlZn"
      },
      "source": [
        "### Assignment 4: TKO_7096-3001 Computer Vision and Sensor Fusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twGxXKt2QlZr"
      },
      "source": [
        "Goal: develop a RGB-Depth fusion architecture for semantic segmentation based on Fully Convolutional Network (FCN) .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2AJ_JULQlZs"
      },
      "source": [
        "<font color='red'> Deadline: 25.03.2024 at 24:00."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "barzq5MdQlZt"
      },
      "source": [
        "- Imports go here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDFUqWn4QlZu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.utils import to_categorical, Sequence\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "from google.colab import files\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smkqqf7dQlZv"
      },
      "source": [
        "### Load the dataset and Ground-truth###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BScHs9BIQlZv"
      },
      "source": [
        "- Dataset consists of 1100 (per modality) images of road scenes. It is divided into train (600 images), test (200 images) and validation (300 images) datasets.\n",
        "- Change the size of all images into 256*256.\n",
        "- Converting the labels into one hot encoding\n",
        "- Create a DataLoader for loading the files when training the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the dataset from your local computer\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract the dataset if it's in a zip file\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"/content/dataset\")  # Change the path as needed\n",
        "        print(f\"Dataset extracted to /content/dataset\")"
      ],
      "metadata": {
        "id": "dx-4IQPiRtG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify extraction\n",
        "extracted_files = os.listdir(\"/content/dataset\")\n",
        "print(\"Extracted files:\", extracted_files)"
      ],
      "metadata": {
        "id": "Dg_bWaWYLWfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Base dataset directory (adjusted for nested structure)\n",
        "base_dir = '/content/dataset/dataset'\n",
        "\n",
        "# Function to get .npy file paths for a specific split (train/test/validation) and modality (rgb/depth/label)\n",
        "def get_npy_paths(split, modality):\n",
        "    return sorted(glob.glob(os.path.join(base_dir, split, modality, '*.npy')))\n",
        "\n",
        "# Load paths for training, validation, and testing datasets\n",
        "rgb_train_paths = get_npy_paths('train', 'rgb')\n",
        "depth_train_paths = get_npy_paths('train', 'depth')\n",
        "label_train_paths = get_npy_paths('train', 'label')\n",
        "\n",
        "rgb_val_paths = get_npy_paths('validation', 'rgb')\n",
        "depth_val_paths = get_npy_paths('validation', 'depth')\n",
        "label_val_paths = get_npy_paths('validation', 'label')\n",
        "\n",
        "rgb_test_paths = get_npy_paths('test', 'rgb')\n",
        "depth_test_paths = get_npy_paths('test', 'depth')\n",
        "label_test_paths = get_npy_paths('test', 'label')\n",
        "\n",
        "# Quick check to confirm paths are loaded\n",
        "print(f\"Train RGB: {len(rgb_train_paths)}, Depth: {len(depth_train_paths)}, Labels: {len(label_train_paths)}\")\n",
        "print(f\"Validation RGB: {len(rgb_val_paths)}, Depth: {len(depth_val_paths)}, Labels: {len(label_val_paths)}\")\n",
        "print(f\"Test RGB: {len(rgb_test_paths)}, Depth: {len(depth_test_paths)}, Labels: {len(label_test_paths)}\")"
      ],
      "metadata": {
        "id": "fpL-axzGUjkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRLrKyEvQlZw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "class DataLoader(Sequence):\n",
        "    def __init__(self, rgb_paths, depth_paths, label_paths, batch_size, num_classes=19, target_size=(256, 256)):\n",
        "        self.rgb_paths = rgb_paths\n",
        "        self.depth_paths = depth_paths\n",
        "        self.label_paths = label_paths\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "        self.target_size = target_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.rgb_paths) / self.batch_size)) # Use floor instead of ceil\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_rgb = self.rgb_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_depth = self.depth_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_labels = self.label_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        return self.__generate_data(batch_rgb, batch_depth, batch_labels)\n",
        "\n",
        "    def __generate_data(self, batch_rgb_files, batch_depth_files, batch_label_files):\n",
        "        rgb_images = np.array([cv2.resize(np.load(file), self.target_size) / 255.0 for file in batch_rgb_files])\n",
        "        depth_images = np.array([cv2.resize(np.concatenate([np.load(file), np.load(file), np.load(file)], axis=-1), self.target_size) / np.max(np.load(file)) if np.max(np.load(file)) != 0 else cv2.resize(np.concatenate([np.load(file), np.load(file), np.load(file)], axis=-1), self.target_size) for file in batch_depth_files])\n",
        "        labels = np.array([cv2.resize(to_categorical(np.load(file), num_classes=self.num_classes), self.target_size) for file in batch_label_files])\n",
        "        labels = labels.reshape(labels.shape[0], -1, labels.shape[-1]) #add this line.\n",
        "\n",
        "        print(\"RGB images shape:\", rgb_images.shape)\n",
        "        print(\"Depth images shape:\", depth_images.shape)\n",
        "        print(\"Labels shape:\", labels.shape)\n",
        "        return (rgb_images, depth_images), labels\n",
        "\n",
        "# ... (rest of your code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFtxwfHsQlZx"
      },
      "source": [
        "- Visualize the data you have prepared"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_samples(rgb_images, depth_images, labels, num_to_display = 3): #add num_to_display\n",
        "    for i in range(min(len(rgb_images), num_to_display)): #display only up to num_to_display images.\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        # Debugging RGB values\n",
        "        print(f\"RGB Image min: {rgb_images[i].min()}, max: {rgb_images[i].max()}, dtype: {rgb_images[i].dtype}\")\n",
        "\n",
        "        # RGB Image (scaling if needed)\n",
        "        rgb_img = rgb_images[i]\n",
        "        if rgb_img.max() <= 1.0:\n",
        "            rgb_img = (rgb_img * 255).astype('uint8')\n",
        "        elif rgb_img.dtype != 'uint8':\n",
        "            rgb_img = rgb_img.astype('uint8')\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.title(\"RGB Image\")\n",
        "        plt.imshow(rgb_img)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Depth Image (grayscale, scaled)\n",
        "        depth_img = depth_images[i]\n",
        "        if depth_img.max() > depth_img.min(): #prevent divide by zero.\n",
        "          depth_img = (depth_img - depth_img.min()) / (depth_img.max() - depth_img.min())  # Scale to [0, 1]\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.title(\"Depth Image\")\n",
        "        plt.imshow(depth_img, cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Label Image\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.title(\"Label\")\n",
        "        plt.imshow(np.argmax(labels[i], axis=-1))\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "JzCXMt07SaAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.load(rgb_train_paths[0]).shape)\n",
        "print(np.load(depth_train_paths[0]).shape)\n",
        "print(np.load(label_train_paths[0]).shape)\n"
      ],
      "metadata": {
        "id": "x3TCZFP9VZz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming rgb_train_paths, depth_train_paths, and label_train_paths are already defined\n",
        "\n",
        "depth_train_data = [np.load(path) for path in depth_train_paths]\n",
        "rgb_train_data = [np.load(path) for path in rgb_train_paths]\n",
        "label_train_data = [np.load(path) for path in label_train_paths]\n",
        "\n",
        "# Modify the depth data\n",
        "modified_depth_data = []\n",
        "for depth_img in depth_train_data:\n",
        "    modified_depth_data.append(np.concatenate([depth_img, depth_img, depth_img], axis=-1))\n",
        "\n",
        "# Modify the RGB Data (optional, but requested by the prompt)\n",
        "modified_rgb_data = []\n",
        "for rgb_img in rgb_train_data:\n",
        "    modified_rgb_data.append(rgb_img) #no change needed, but the prompt says to copy it.\n",
        "\n",
        "#Verify the result\n",
        "print(modified_depth_data[0].shape)\n",
        "print(modified_rgb_data[0].shape)\n",
        "print(label_train_data[0].shape)\n",
        "\n",
        "# Example usage with a visualization function (if you have one):\n",
        "# visualize_samples(modified_rgb_data, modified_depth_data, label_train_data)"
      ],
      "metadata": {
        "id": "41Q60eccS_bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 3  # For visualization\n",
        "data_loader = DataLoader(rgb_train_paths, depth_train_paths, label_train_paths, batch_size)\n",
        "\n",
        "# Get the first batch\n",
        "(data_rgb, data_depth), data_labels = data_loader[0]\n",
        "\n",
        "# Visualize the first batch\n",
        "visualize_samples(data_rgb, data_depth, data_labels)"
      ],
      "metadata": {
        "id": "bK1BVImXUSG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsS2PQbeQlZx"
      },
      "source": [
        " Define a Fully Convolutional Network (FCN) for image segmentaion by fusing RGB and depth images. The network consists of  two sterams which each stream has following layers:\n",
        "\n",
        "    1. Use the pretrained ResNet50 on imageNet\n",
        "    2. Add two Conv layers with 128 and 256 nodes, respectively. Kernel size (3,3), stride (1,1)\n",
        "    3. Top of the Conv layers, add dropout layer with 0.2\n",
        "    4. Concatenate two streams.\n",
        "    5. Add a transposed convolution layer (Conv2DTranspose)  with Kernel size (64,64), stride (32,32)\n",
        "    6. Add a softmax activation layer\n",
        "    \n",
        " You can find the model summary and structure in the PDF file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXkOkiUZQlZy"
      },
      "outputs": [],
      "source": [
        "class UniqueResNet50(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(UniqueResNet50, self).__init__(**kwargs)\n",
        "        self.resnet = ResNet50(weights='imagenet', include_top=False)\n",
        "        for layer in self.resnet.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.resnet(inputs)\n",
        "\n",
        "def build_fcn_segmentation_model(input_shape=(256, 256, 3), num_classes=19):\n",
        "    \"\"\"Builds a Fully Convolutional Network (FCN) for image segmentation.\"\"\"\n",
        "\n",
        "    # Input layers\n",
        "    input_rgb = layers.Input(shape=input_shape, name=\"input_rgb\")\n",
        "    input_depth = layers.Input(shape=input_shape, name=\"input_depth\")\n",
        "\n",
        "    # ResNet50 base models (pretrained) with unique instances\n",
        "    rgb_stream = UniqueResNet50()(input_rgb)\n",
        "    depth_stream = UniqueResNet50()(input_depth)\n",
        "\n",
        "    # RGB stream\n",
        "    rgb_stream = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(rgb_stream)\n",
        "    rgb_stream = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(rgb_stream)\n",
        "    rgb_stream = layers.Dropout(0.2)(rgb_stream)\n",
        "\n",
        "    # Depth stream\n",
        "    depth_stream = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(depth_stream)\n",
        "    depth_stream = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(depth_stream)\n",
        "    depth_stream = layers.Dropout(0.2)(depth_stream)\n",
        "\n",
        "    # Concatenate streams\n",
        "    concatenated = layers.Concatenate()([rgb_stream, depth_stream])\n",
        "\n",
        "    # Transposed convolution (upsampling)\n",
        "    upsampled = layers.Conv2DTranspose(num_classes, (64, 64), strides=(32, 32), padding='same')(concatenated)\n",
        "\n",
        "    # Reshape (optional, for compatibility with some loss functions)\n",
        "    reshaped = layers.Reshape((-1, num_classes))(upsampled)\n",
        "\n",
        "    # Softmax activation\n",
        "    output = layers.Activation('softmax')(reshaped)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=[input_rgb, input_depth], outputs=output)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = build_fcn_segmentation_model()"
      ],
      "metadata": {
        "id": "VeqRfrx5e-SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "O9uatZtxfDkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Assuming 'model' is your Keras model object\n",
        "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "UnpWBA4uV9Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe9beqMuQlZy"
      },
      "source": [
        "Compile the model with SGD(learning_rate=0.01, decay=1e-5, momentum=0.9) and loss=\"categorical_crossentropy\"\n",
        "\n",
        "Train the model on the “train” dataset  and “validation”dataset for epochs =10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AjvJ4KjQlZy"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "optimizer = optimizers.SGD(learning_rate=0.01, decay=1e-5, momentum=0.9)\n",
        "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(rgb_train_paths, depth_train_paths, label_train_paths, batch_size)\n",
        "val_loader = DataLoader(rgb_val_paths, depth_val_paths, label_val_paths, batch_size)\n"
      ],
      "metadata": {
        "id": "2_Pt-uYUcWDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator function to iterate through DataLoader\n",
        "def generator(data_loader):\n",
        "  for idx in range(len(data_loader)):\n",
        "    yield data_loader.__getitem__(idx)"
      ],
      "metadata": {
        "id": "sv6GzYdHguI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the DataLoaders to TensorFlow Datasets with explicit types and shapes\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: generator(train_loader), # Use the generator function\n",
        "    output_types=((tf.float32, tf.float32), tf.float32),\n",
        "    output_shapes=(((batch_size, 256, 256, 3), (batch_size, 256, 256, 3)), (batch_size, 65536, 19))\n",
        ")\n",
        "val_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: generator(val_loader), # Use the generator function\n",
        "    output_types=((tf.float32, tf.float32), tf.float32),\n",
        "    output_shapes=(((batch_size, 256, 256, 3), (batch_size, 256, 256, 3)), (batch_size, 65536, 19))\n",
        ")"
      ],
      "metadata": {
        "id": "rdl7pU1FgJEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs = 10\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=epochs)"
      ],
      "metadata": {
        "id": "jeBzfgTQcaTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WZrx8_ekcYB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5F6PF1gQlZy"
      },
      "source": [
        " Evaluate the model on the training and test dataset. The results must be shown as\n",
        "\n",
        "- Print loss and accuracy of model for  test dataset.\n",
        "\n",
        "- Predict semantically segmented images on 5 random example of test dataset.\n",
        "\n",
        "- Visualize the 5 random examples alongside the ground truth and prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv-h9XBaQlZy"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test dataset\n",
        "loss, accuracy = model.evaluate(val_dataset)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict semantically segmented images on 5 random examples of the test dataset\n",
        "num_samples = 5\n",
        "random_indices = random.sample(range(len(rgb_val_paths)), num_samples)"
      ],
      "metadata": {
        "id": "KhYUNZOHp5f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in random_indices:\n",
        "    rgb_image = np.load(rgb_val_paths[i]) / 255.0\n",
        "    depth_image = np.concatenate([np.load(depth_val_paths[i]), np.load(depth_val_paths[i]), np.load(depth_val_paths[i])], axis=-1) / np.max(np.load(depth_val_paths[i])) if np.max(np.load(depth_val_paths[i])) != 0 else np.concatenate([np.load(depth_val_paths[i]), np.load(depth_val_paths[i]), np.load(depth_val_paths[i])], axis=-1)\n",
        "    ground_truth = np.load(label_val_paths[i])\n",
        "\n",
        "    # Resize images to match model input shape\n",
        "    rgb_image = cv2.resize(rgb_image, (256, 256))\n",
        "    depth_image = cv2.resize(depth_image, (256, 256))\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict([np.expand_dims(rgb_image, axis=0), np.expand_dims(depth_image, axis=0)])\n",
        "    predicted_labels = np.argmax(prediction, axis=2).reshape((256, 256))\n",
        "\n",
        "    # Visualize the results\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(rgb_image)\n",
        "    plt.title(\"RGB Image\")\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(ground_truth)\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(predicted_labels)\n",
        "    plt.title(\"Predicted Labels\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "R3t7SVwEp5wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um9WuzX7QlZz"
      },
      "source": [
        "### Extra 5 points :\n",
        "\n",
        "Implement FCNs for each sing modality and compare their accuracy with fusion model. I need the result of the following table in the same notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qx0yGfcQlZz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHP2kGhJQlZ0"
      },
      "source": [
        "# RESULT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeQPghi7QlZ0"
      },
      "source": [
        "| **Modality** | **Test Accuracy(%)** |\n",
        "| -------- | -------- |\n",
        "| RGB only |  |\n",
        "| Depth Only |  |\n",
        "| RBB and Depth Fusion |  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taJW3YNQQlZ1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4hADfkvJfyaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Model, optimizers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Assuming rgb_train_paths, depth_train_paths, label_train_paths,\n",
        "# rgb_val_paths, depth_val_paths, label_val_paths are already defined\n",
        "\n",
        "class UniqueResNet50(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(UniqueResNet50, self).__init__(**kwargs)\n",
        "        self.resnet = ResNet50(weights='imagenet', include_top=False)\n",
        "        for layer in self.resnet.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.resnet(inputs)\n",
        "\n",
        "def build_fcn_segmentation_model(input_shape=(256, 256, 3), num_classes=19):\n",
        "    \"\"\"Builds a Fully Convolutional Network (FCN) for image segmentation.\"\"\"\n",
        "\n",
        "    # Input layers\n",
        "    input_rgb = layers.Input(shape=input_shape, name=\"input_rgb\")\n",
        "    input_depth = layers.Input(shape=input_shape, name=\"input_depth\")\n",
        "\n",
        "    # ResNet50 base models (pretrained) with unique instances\n",
        "    rgb_stream = UniqueResNet50()(input_rgb)\n",
        "    depth_stream = UniqueResNet50()(input_depth)\n",
        "\n",
        "    # RGB stream\n",
        "    rgb_stream = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(rgb_stream)\n",
        "    rgb_stream = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(rgb_stream)\n",
        "    rgb_stream = layers.Dropout(0.2)(rgb_stream)\n",
        "\n",
        "    # Depth stream\n",
        "    depth_stream = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(depth_stream)\n",
        "    depth_stream = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(depth_stream)\n",
        "    depth_stream = layers.Dropout(0.2)(depth_stream)\n",
        "\n",
        "    # Concatenate streams\n",
        "    concatenated = layers.Concatenate()([rgb_stream, depth_stream])\n",
        "\n",
        "    # Transposed convolution (upsampling)\n",
        "    upsampled = layers.Conv2DTranspose(num_classes, (64, 64), strides=(32, 32), padding='same')(concatenated)\n",
        "\n",
        "    # Reshape (optional, for compatibility with some loss functions)\n",
        "    reshaped = layers.Reshape((-1, num_classes))(upsampled)\n",
        "\n",
        "    # Softmax activation\n",
        "    output = layers.Activation('softmax')(reshaped)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=[input_rgb, input_depth], outputs=output)\n",
        "    return model\n",
        "\n",
        "class DataLoader(Sequence):\n",
        "    def __init__(self, rgb_paths, depth_paths, label_paths, batch_size, num_classes=19):\n",
        "        self.rgb_paths = rgb_paths\n",
        "        self.depth_paths = depth_paths\n",
        "        self.label_paths = label_paths\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.rgb_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_rgb = self.rgb_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_depth = self.depth_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_labels = self.label_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        return self.__generate_data(batch_rgb, batch_depth, batch_labels)\n",
        "\n",
        "    def __generate_data(self, batch_rgb_files, batch_depth_files, batch_label_files):\n",
        "        rgb_images = np.array([np.load(file) / 255.0 for file in batch_rgb_files])  # Normalize RGB\n",
        "        depth_images = np.array([np.concatenate([np.load(file), np.load(file), np.load(file)], axis=-1) / np.max(np.load(file)) if np.max(np.load(file)) != 0 else np.concatenate([np.load(file), np.load(file), np.load(file)], axis=-1) for file in batch_depth_files])  # Normalize depth and duplicate channel\n",
        "        labels = np.array([to_categorical(np.load(file), num_classes=self.num_classes) for file in batch_label_files])\n",
        "        return [rgb_images, depth_images], labels\n",
        "\n",
        "# Build the model\n",
        "model = build_fcn_segmentation_model()\n",
        "\n",
        "# Compile the model\n",
        "optimizer = optimizers.SGD(learning_rate=0.01, decay=1e-5, momentum=0.9)\n",
        "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 16 #adjust as needed.\n",
        "train_loader = DataLoader(rgb_train_paths, depth_train_paths, label_train_paths, batch_size)\n",
        "val_loader = DataLoader(rgb_val_paths, depth_val_paths, label_val_paths, batch_size)\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "model.fit(train_loader, validation_data=val_loader, epochs=epochs)"
      ],
      "metadata": {
        "id": "ob--icU5fydE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}